{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEKy3XmT7H-U"
   },
   "source": [
    "# # Classifiers introduction\n",
    "\n",
    "In the following program we introduce the basic steps of classification of a dataset in a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btrnWCpJ7H-W"
   },
   "source": [
    "Import the package for learning and modeling trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QFXKb7yA7H-X",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sx9T_yz7H-f"
   },
   "source": [
    "Define the matrix containing the data (one example per row)\n",
    "and the vector containing the corresponding target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HDFiRDXq7H-g"
   },
   "outputs": [],
   "source": [
    "X = [[0, 0, 0], [1, 1, 1], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]]\n",
    "Y = [1, 0, 0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLgMBwpZ7H-n"
   },
   "source": [
    "Declare the classification model you want to use and then fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RbHJM2ur7H-o"
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUtyP-xO7H-t"
   },
   "source": [
    "Predict the target value (and print it) for the passed data, using the fitted model currently in clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RTo8XvPr7H-v",
    "outputId": "f2c67b3c-9ce6-4dfb-e69f-ce9120823c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[0, 1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ncK8CxO17H-0",
    "outputId": "5f2abb14-eb54-4b2e-b1af-cdc13bc6a40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([[1, 0, 1],[0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qP9sNQ2t7H-5",
    "outputId": "7b57e3de-e2a2-4b38-f699-c6c2106e1bf0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5ed7f8b048e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdot_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oCMYXsO7H--"
   },
   "source": [
    "In the following we start using a dataset (from UCI Machine Learning repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QICMJ0rv7H-_"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzZnLA4N7H_D"
   },
   "source": [
    "# Declare the type of prediction model and the working criteria for the model induction algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ul-cA4Hv7H_E"
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqH8H1UD7H_H"
   },
   "source": [
    "# Split the dataset in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSKsWOoz7H_I"
   },
   "outputs": [],
   "source": [
    "# Generate a random permutation of the indices of examples that will be later used \n",
    "# for the training and the test set\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris.data))\n",
    "\n",
    "# We now decide to keep the last 10 indices for test set, the remaining for the training set\n",
    "indices_training=indices[:-10]\n",
    "indices_test=indices[-10:]\n",
    "\n",
    "iris_X_train = iris.data[indices_training] # keep for training all the matrix elements with the exception of the last 10 \n",
    "iris_y_train = iris.target[indices_training]\n",
    "iris_X_test  = iris.data[indices_test] # keep the last 10 elements for test set\n",
    "iris_y_test  = iris.target[indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNbXUYI07H_K"
   },
   "outputs": [],
   "source": [
    "len(iris.data)\n",
    "len(iris_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A3MqXZs7H_L"
   },
   "outputs": [],
   "source": [
    "# fit the model to the training data\n",
    "clf = clf.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPdjlWZB7H_P"
   },
   "source": [
    "# Obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMP6rS-97H_P",
    "outputId": "e0f9707c-5465-4128-9eb2-0c43f4a3f75b"
   },
   "outputs": [],
   "source": [
    "# apply fitted model \"clf\" to the test set \n",
    "predicted_y_test = clf.predict(iris_X_test)\n",
    "\n",
    "# print the predictions (class numbers associated to classes names in target names)\n",
    "print(\"Predictions:\")\n",
    "print(predicted_y_test)\n",
    "print(\"True classes:\")\n",
    "print(iris_y_test) \n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TZyuYLx7H_T"
   },
   "source": [
    "Print the index of the test instances and the corresponding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRz0f9NK7H_T",
    "outputId": "4a04bd5b-ee05-491d-8cac-adcfd17fb81f"
   },
   "outputs": [],
   "source": [
    "# print the corresponding instances indexes and class names \n",
    "for i in range(len(iris_y_test)): \n",
    "    print(\"Instance # \"+str(indices_test[i])+\": \")\n",
    "    print(\"Predicted: \"+iris.target_names[predicted_y_test[i]]+\"\\t True: \"+iris.target_names[iris_y_test[i]]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yj3hYAqQ7H_W"
   },
   "source": [
    "# Look at the specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGKB-S6b7H_X",
    "outputId": "819c7ba9-56a8-41dc-f61e-d59a1a893ffe"
   },
   "outputs": [],
   "source": [
    "for i in range(len(iris_y_test)): \n",
    "    print(\"Instance # \"+str(indices_test[i])+\": \")\n",
    "    s=\"\"\n",
    "    for j in range(len(iris.feature_names)):\n",
    "        s=s+iris.feature_names[j]+\"=\"+str(iris_X_test[i][j])\n",
    "        if (j<len(iris.feature_names)-1): s=s+\", \"\n",
    "    print(s)\n",
    "    print(\"Predicted: \"+iris.target_names[predicted_y_test[i]]+\"\\t True: \"+iris.target_names[iris_y_test[i]]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9zos0wm7H_Z"
   },
   "source": [
    "# Obtain model performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBNKQQqa7H_a",
    "outputId": "09c73bd7-5f55-4409-8958-0705aa4b73be"
   },
   "outputs": [],
   "source": [
    "# print some metrics results\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "acc_score = accuracy_score(iris_y_test, predicted_y_test)\n",
    "print(\"Accuracy score: \"+ str(acc_score))\n",
    "f1=f1_score(iris_y_test, predicted_y_test, average='macro')\n",
    "print(\"F1 score: \"+str(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNO0xcGq7H_c"
   },
   "source": [
    "# Use Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmt414Lm7H_c",
    "outputId": "da09947d-d490-4717-cd7c-c2c544a0d261"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score # will be used to separate training and test\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5) # score will be the accuracy\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-rd5Dq77H_e",
    "outputId": "0c1f75ee-948a-4184-a9f1-7c10a7747aeb"
   },
   "outputs": [],
   "source": [
    "# computes F1- score\n",
    "f1_scores = cross_val_score(clf, iris.data, iris.target, cv=5, scoring='f1_macro')\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcnegz_A7H_h"
   },
   "source": [
    "# Show the resulting tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrA-f0e27H_i"
   },
   "source": [
    "## 1. Print the picture in a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPYEWkcH7H_i",
    "outputId": "28d4261a-7ad1-47a1-ab5d-a0583bfc014d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"my_iris_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM67N3vc7H_k"
   },
   "source": [
    "## 2. Generate a picture here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prgjDOka7H_l",
    "outputId": "2263ea70-f260-4933-80f3-63bc23d00ace"
   },
   "outputs": [],
   "source": [
    "print(list(iris.feature_names))\n",
    "print(list(iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pr32sBno7H_n",
    "outputId": "6f741234-c217-448b-b711-ed46e541e862"
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names, \n",
    "                         class_names=iris.target_names, \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90GvIMjgeUue"
   },
   "source": [
    "# Your work: what you have to do\n",
    "Modify the given Jupyter notebook on decision trees on Iris data and perform the following tasks:\n",
    "\n",
    "1. get an artificial inflation of some class in the training set by a given factor: 10 (weigh more the classes virginica e versicolor which are more difficult to discriminate). Learn the tree in these conditions.\n",
    "1.b) modify the weight of some classes (set to 10 the weights for misclassification between virginica into versicolor and vice versa) and learn the tree in these conditions. You should obtain similar results as for step 1.\n",
    "2. learn trees but try to avoid overfitting (by improving the error on the test set) tuning the hyper-parameters on: the minimum number of samples per leaf, max depth of the tree, min_impurity_decrease parameters, max leaf nodes, etc.\n",
    "3. build the confusion matrix of the created tree models on the test set and show them. \n",
    "4. build the ROC curves (or coverage curves in coverage space) and plot them for each tree model you have created: for each model you have to build three curves, one for each class, considered in turn as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    l = list(zip(X, y))\n",
    "    np.random.shuffle(l)\n",
    "    return list(zip(*l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzHbGJsTOoQU"
   },
   "source": [
    "## 1.a Artificial inflation\n",
    "The classes virginica and versicolor have been oversampled by a factor of 10. Each example belonging to those classses has been repeated 10 times in order to achieve the artificial inflation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "inflated_data = iris.data \n",
    "inflated_target = iris.target\n",
    "training_test_size = 300\n",
    "\n",
    "inf_indexes = []\n",
    "for i in range(len(iris.target)):\n",
    "    if (iris.target[i] != 0):\n",
    "        inf_indexes += [i]\n",
    "\n",
    "for j in inf_indexes:\n",
    "    for i in range(9):\n",
    "        inflated_target = np.append(inflated_target, [iris.target[j]], axis=0)\n",
    "        inflated_data = np.append(inflated_data, [iris.data[j]], axis=0)\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(inflated_data))\n",
    "\n",
    "# We now decide to keep the last 10 indices for test set, the remaining for the training set\n",
    "indices_training=indices[:-training_test_size]\n",
    "indices_test=indices[-training_test_size:]\n",
    "\n",
    "iris_X_train_inflated = inflated_data[indices_training] # keep for training all the matrix elements with the exception of the last 10 \n",
    "iris_y_train_inflated = inflated_target[indices_training]\n",
    "iris_X_test_inflated  = inflated_data[indices_test] # keep the last 10 elements for test set\n",
    "iris_y_test_inflated  = inflated_target[indices_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_inf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:1,2:1})\n",
    "clf_inf = clf_inf.fit(iris_X_train_inflated, iris_y_train_inflated)\n",
    "\n",
    "predicted_y_test_inflated = clf_inf.predict(iris_X_test_inflated)\n",
    "\n",
    "# print some metrics results\n",
    "acc_score = accuracy_score(iris_y_test_inflated, predicted_y_test_inflated)\n",
    "print(\"Accuracy score: \"+ str(acc_score))\n",
    "f1=f1_score(iris_y_test_inflated, predicted_y_test_inflated, average='macro')\n",
    "print(\"F1 score: \"+str(f1))\n",
    "cross_scores_inflated = cross_val_score(clf_inf, iris_X_train_inflated, iris_y_train_inflated, cv=5) # score will be the accuracy\n",
    "print(\"Cross-validation accuracy: \"+str(cross_scores_inflated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf_inf, out_file=None, \n",
    "                         feature_names=iris.feature_names, \n",
    "                         class_names=iris.target_names, \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b Unbalanced weights\n",
    "Weights for misclassification of the classes virginica and versicolor have been inhcreased by a factor of 10 to achieve similar results to the oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "test_size = 10\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris.data))\n",
    "\n",
    "# We now decide to keep the last 10 indices for test set, the remaining for the training set\n",
    "indices_training=indices[:-test_size]\n",
    "indices_test=indices[-test_size:]\n",
    "\n",
    "iris_X_train = iris.data[indices_training] # keep for training all the matrix elements with the exception of the last 10 \n",
    "iris_y_train = iris.target[indices_training]\n",
    "iris_X_test  = iris.data[indices_test] # keep the last 10 elements for test set\n",
    "iris_y_test  = iris.target[indices_test]\n",
    "\n",
    "clf_unb = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=10, class_weight={0:1,1:10,2:10})\n",
    "clf_unb = clf_unb.fit(iris_X_train, iris_y_train)\n",
    "\n",
    "predicted_y_test_unbalanced = clf_unb.predict(iris_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some metrics results\n",
    "acc_score = accuracy_score(iris_y_test, predicted_y_test_unbalanced)\n",
    "print(\"Accuracy score: \"+ str(acc_score))\n",
    "f1=f1_score(iris_y_test, predicted_y_test_unbalanced, average='macro')\n",
    "print(\"F1 score: \"+str(f1))\n",
    "cross_scores_unbalanced = cross_val_score(clf_unb, iris.data, iris.target, cv=5) # score will be the accuracy\n",
    "print(\"Cross-validation accuracy: \"+str(cross_scores_unbalanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf_unb, out_file=None, \n",
    "                         feature_names=iris.feature_names, \n",
    "                         class_names=iris.target_names, \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Try to avoid overfitting\n",
    "Learning trees but trying to avoid overfitting:\n",
    "- Minimum number of samples per leaf: comparing cross-validation accuracy scores emerges that the the model which uses unbalanced weights is not affected by this hyperparameter, while the optimal setting for the one which uses artificial inflation seems to be 1, although 11 gives great results and I'm prone to think it's a safer choice to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    'criterion': [\"entropy\",\"gini\"], \n",
    "    #'min_samples_leaf': range(1,30),\n",
    "    'max_depth': range(1,10),\n",
    "    'max_leaf_nodes': range(3,15),\n",
    "    'min_impurity_decrease': np.arange(0,1,0.1),\n",
    "    'max_features': [\"auto\", \"sqrt\", \"log2\", None]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search for the artificial inflated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_inf = GridSearchCV(clf_inf, param_grid, cv=5)\n",
    "search_inf.fit(iris_X_train_inflated, iris_y_train_inflated)\n",
    "search_inf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"means = search.cv_results_['mean_test_score']\n",
    "stds = search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" \n",
    "    % (mean, std * 2, params))\n",
    "print()\"\"\"\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = iris_y_test_inflated, search_inf.predict(iris_X_test_inflated)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search for the unbalanced weighted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_unb = GridSearchCV(clf_unb, param_grid, cv=5)\n",
    "search_unb.fit(iris_X_train, iris_y_train)\n",
    "search_unb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"means = search.cv_results_['mean_test_score']\n",
    "stds = search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" \n",
    "    % (mean, std * 2, params))\n",
    "print()\"\"\"\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_true, y_pred = iris_y_test, search_unb.predict(iris_X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "g = \"#7de391\"\n",
    "r = \"#c94040\"\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Tp',\n",
    "                          markerfacecolor=g, markersize=10),\n",
    "                   Line2D([0],[0],marker='o', color='w', label='Fp',\n",
    "                          markerfacecolor=r, markersize=10)]\n",
    "\n",
    "cm = confusion_matrix(iris_y_test, predicted_y_test_unbalanced, labels=[0,1,2])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "colors = [[g, r, r],[r, g, r],[r, r, g]]\n",
    "the_table = ax.table(rowLabels=iris.target_names, colLabels=iris.target_names, cellText=cm, cellColours=colors, loc=\"center\")\n",
    "ax.legend(handles=legend_elements)\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.title(\"Confusion matrix\", loc=\"left\")\n",
    "the_table.scale(1,1.5)\n",
    "\n",
    "plt.box(on=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "g = \"#7de391\"\n",
    "r = \"#c94040\"\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Tp',\n",
    "                          markerfacecolor=g, markersize=10),\n",
    "                   Line2D([0],[0],marker='o', color='w', label='Fp',\n",
    "                          markerfacecolor=r, markersize=10)]\n",
    "\n",
    "cm = confusion_matrix(iris_y_test_inflated, predicted_y_test_inflated, labels=[0,1,2])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "colors = [[g, r, r],[r, g, r],[r, r, g]]\n",
    "the_table = ax.table(rowLabels=iris.target_names, colLabels=iris.target_names, cellText=cm, cellColours=colors, loc=\"center\")\n",
    "ax.legend(handles=legend_elements)\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.title(\"Confusion matrix\", loc=\"left\")\n",
    "the_table.scale(1,1.5)\n",
    "\n",
    "plt.box(on=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "legend_elements = [ iris.target_names[0] + \" vs \" + iris.target_names[1] + \" and \" + iris.target_names[2],\n",
    "                    iris.target_names[1] + \" vs \" + iris.target_names[0] + \" and \" + iris.target_names[2],\n",
    "                    iris.target_names[2] + \" vs \" + iris.target_names[0] + \" and \" + iris.target_names[1]]\n",
    "\n",
    "\n",
    "for t in [0,1,2]:\n",
    "    y_binary = [0 if y == t else 1 for y in iris_y_test]\n",
    "    y_pred_binary = [0 if y == t else 1 for y in predicted_y_test_unbalanced]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_binary, y_pred_binary, drop_intermediate=False)\n",
    "    ax1.plot(fpr, tpr)\n",
    "\n",
    "ax1.legend(legend_elements)\n",
    "ax1.title.set_text(\"Unbalanced weights\")\n",
    "\n",
    "for t in [0,1,2]:\n",
    "    y_binary = [0 if y == t else 1 for y in iris_y_test_inflated]\n",
    "    y_pred_binary = [0 if y == t else 1 for y in predicted_y_test_inflated]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_binary, y_pred_binary, drop_intermediate=False)\n",
    "    ax2.plot(fpr, tpr)\n",
    "\n",
    "ax2.legend(legend_elements)\n",
    "ax2.title.set_text(\"Inflated data\")\n",
    "\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(4)\n",
    "fig.suptitle(\"ROC comparison\", )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy wrt different size of the leafs in a model with unbalanced weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "res_cross = np.zeros(len(iris_X_train))\n",
    "res_pred = np.zeros(len(iris_X_train))\n",
    "\n",
    "for leafs in range(len(iris_X_train)):\n",
    "    clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=leafs+1, class_weight={0:1,1:10,2:10})\n",
    "    clf.fit(iris_X_train, iris_y_train)\n",
    "    preds = clf.predict(iris_X_test)\n",
    "    res_pred[leafs] = accuracy_score(iris_y_test, preds)\n",
    "    res_cross[leafs] = cross_val_score(clf, iris_X_train, iris_y_train).mean()\n",
    "\n",
    "fif, ax = plt.subplots()\n",
    "ax.plot(range(len(iris_X_train))[:50], res_cross[:50])\n",
    "ax.plot(range(len(iris_X_train))[:50], res_pred[:50])\n",
    "ax.legend([\"Cross-validation accuracy\", \"Prediction accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=10, class_weight={0:1,1:10,2:10})\n",
    "clf.fit(iris_X_train, iris_y_train)\n",
    "preds = clf.predict(iris_X_test)\n",
    "a = accuracy_score(iris_y_test, preds)\n",
    "c = cross_val_score(clf, iris_X_train, iris_y_train)\n",
    "print(\"Acc: \" + str(a) + \" Cross: \" + str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy wrt different size of the leafs in a model with inflated classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "res_cross = np.zeros(len(iris_X_train))\n",
    "res_pred = np.zeros(len(iris_X_train))\n",
    "\n",
    "for leafs in range(len(iris_X_train)):\n",
    "    clf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=leafs+1, class_weight={0:1,1:1,2:1})\n",
    "    clf.fit(iris_X_train_inflated, iris_y_train_inflated)\n",
    "    preds = clf.predict(iris_X_test_inflated)\n",
    "    res_pred[leafs] = accuracy_score(iris_y_test_inflated, preds)\n",
    "    res_cross[leafs] = cross_val_score(clf, iris_X_train_inflated, iris_y_train_inflated).mean()\n",
    "\n",
    "fif, ax = plt.subplots()\n",
    "ax.plot(range(len(iris_X_train))[:50], res_cross[:50])\n",
    "ax.plot(range(len(iris_X_train))[:50], res_pred[:50])\n",
    "ax.legend([\"Cross-validation accuracy\", \"Prediction accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation accuracy score for different number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "k_folds = [3,5,7,9]\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "min_accuracy = 0.93\n",
    "plt.title(\"Cross-validation accuracy score for different number of folds\")\n",
    "fig.set_figwidth(13)\n",
    "\n",
    "for k in k_folds :\n",
    "    max_cross = 0\n",
    "    best_min_leafs = 1\n",
    "    scores = np.zeros(len(inflated_data))\n",
    "\n",
    "    for min_leafs in range(len(inflated_data)):\n",
    "        clf_inf = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=min_leafs+1,class_weight={0:1,1:1,2:1})\n",
    "        cross_scores_inflated = cross_val_score(clf_inf, iris_X_train_inflated, iris_y_train_inflated, cv=k) # score will be the accuracy\n",
    "        scores[min_leafs] = cross_scores_inflated.mean()\n",
    "\n",
    "        if(max_cross <= scores[min_leafs]):\n",
    "            max_cross = scores[min_leafs]\n",
    "            best_min_leafs = min_leafs+1\n",
    "\n",
    "    #print(\"Best mean cross score: \" + str(max_cross) + \" with \" + str(best_min_leafs) + \" minimum leafs and \" + str(k) + \" folds\")\n",
    "\n",
    "    high_scores = scores[scores>=min_accuracy]\n",
    "    x_high_scores = np.arange(len(inflated_data))[scores>=min_accuracy]\n",
    "    ax1.plot(x_high_scores[::-1], high_scores[::-1])\n",
    "\n",
    "ax1.legend([str(k) + \" folds\" for k in k_folds])\n",
    "ax1.set_ylabel(\"Mean cross-validation accuracy score\")\n",
    "ax1.set_xlabel(\"Minimum sample leafs\")\n",
    "ax1.title.set_text(\"Artificial inflated dataset\")\n",
    "\n",
    "################################################################################################################################\n",
    "for k in k_folds :\n",
    "    max_cross = 0\n",
    "    best_min_leafs = 1\n",
    "    scores = np.zeros(len(iris.data))\n",
    "\n",
    "    for min_leafs in range(len(iris.data)):\n",
    "        clf_un = tree.DecisionTreeClassifier(criterion=\"entropy\",random_state=300,min_samples_leaf=5,class_weight={0:1,1:10,2:10})\n",
    "        cross_scores_inflated = cross_val_score(clf_un, iris_X_train, iris_y_train, cv=k) # score will be the accuracy\n",
    "        scores[min_leafs] = cross_scores_inflated.mean()\n",
    "\n",
    "        if(max_cross <= scores[min_leafs]):\n",
    "            max_cross = scores[min_leafs]\n",
    "            best_min_leafs = min_leafs+1\n",
    "\n",
    "    #print(\"Best mean cross score: \" + str(max_cross) + \" with \" + str(best_min_leafs) + \" minimum leafs and \" + str(k) + \" folds\")\n",
    "\n",
    "    high_scores = scores[scores>min_accuracy]\n",
    "    x_high_scores = np.arange(len(iris.data))[scores>=min_accuracy]\n",
    "    ax2.plot(x_high_scores[::-1], high_scores[::-1])\n",
    "\n",
    "ax2.legend([str(k) + \" folds\" for k in k_folds])\n",
    "ax2.set_ylabel(\"Mean cross-validation accuracy score\")\n",
    "ax2.set_xlabel(\"Minimum sample leafs\")\n",
    "ax2.title.set_text(\"Unbalanced weights dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "classification_iris_aa_19_20.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
